{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2769c7a-77ae-497d-b88f-3e1c3b555d7f",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d9a3c-adf6-46ee-83ac-6f7adf2c3e2f",
   "metadata": {},
   "source": [
    "#### Based on the insights generated from the exploratory data analysis, we will built the features that would work for the dataset and our usecase objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b584d-d80f-403e-9382-6a9eb8b87b64",
   "metadata": {},
   "source": [
    "#### Features created in different tiers considering latency & complexity:\n",
    "* Tier 1: Simple features & Fast calculation\n",
    "* Tier 2: Signal Quality features & Moderate\n",
    "* Tier 3: Advanced Features & Slower\n",
    "* Power System Specific features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c305b62-d4f0-432c-a41a-d657062da279",
   "metadata": {},
   "source": [
    "### Steps for creating the features:\n",
    "* __Subset Selection__: 600K out of 800K measurements (75%) per signal and 5K/8.7K signals are selected to be memory and time efficient \n",
    "* __Chunk Signal__: For Feature calculation, each 600K measurements per signal is divded  into chunks of 100 measurements for easier calculation\n",
    "* __Feature creation__ :\n",
    "\t* Features across all tiers are created at chunk level\n",
    "\t* Features across all chunks are aggregated back to signal level using statistical measures – std,mean etc.\n",
    "* __Preprocessing & Feature Selection__ :\n",
    "\t* Handling missing /infinite values and outliers\n",
    "\t* Selecting High Variance Features\n",
    "\t* Scaling with RobustScaler in presence of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff40bd47-37a2-4ca4-bd0b-5c16ce44a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import find_peaks, periodogram, welch\n",
    "import pywt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08765497-d6ef-45c5-a6b7-bcc5cb6786eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'./data/vsb-power-line-fault-detection/'\n",
    "FEATURE_PATH = r\"./features//\"\n",
    "CHUNK_SIZE = 1000\n",
    "SAMPLE_SIZE=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "375b02bd-1348-4ddb-b6b0-f9f0e80b884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df  = pd.read_csv(f\"{DATA_PATH}/metadata_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da100cca-c76a-4c95-bd79-b679d55a73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 53.2 s\n",
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_path = f\"{DATA_PATH}/train.parquet\"\n",
    "signals_df = pd.read_parquet(train_path, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361cba3-26ff-4037-a5c3-bdb46d204892",
   "metadata": {},
   "source": [
    "### Modules for feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f30a7-a805-494b-8982-058d6b53f726",
   "metadata": {},
   "source": [
    "#### 1. Basic Features (Fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac055be8-8bdc-4c5d-8020-6b9980960435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_basic_features(signal_chunk):\n",
    "    \"\"\"\n",
    "    Extract Basic features: Faster statistical features (< 1ms)\n",
    "    These are the most discriminative and fastest to compute\n",
    "    \n",
    "    Args:\n",
    "        signal_chunk: 1D array of signal values\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of Tier 1 features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Core statistical features - highest discriminative power from EDA\n",
    "    features['std'] = np.std(signal_chunk)  # Most discriminative single feature\n",
    "    features['rms'] = np.sqrt(np.mean(signal_chunk**2))  # Power system standard\n",
    "    features['mean_abs'] = np.mean(np.abs(signal_chunk))\n",
    "    \n",
    "    # Amplitude-based features - clear separation observed in EDA\n",
    "    features['min_val'] = np.min(signal_chunk)\n",
    "    features['max_val'] = np.max(signal_chunk)\n",
    "    features['peak_to_peak'] = features['max_val'] - features['min_val']\n",
    "    \n",
    "    # Power system specific ratios - efficient fault indicators\n",
    "    features['crest_factor'] = features['max_val'] / features['rms'] if features['rms'] > 1e-10 else 0\n",
    "    features['form_factor'] = features['rms'] / features['mean_abs'] if features['mean_abs'] > 1e-10 else 0\n",
    "    \n",
    "    # Deviation measures - fault signals showed higher variability\n",
    "    features['mean_abs_deviation'] = np.mean(np.abs(signal_chunk - np.mean(signal_chunk)))\n",
    "    features['variance'] = np.var(signal_chunk)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121c22a-438e-4a8f-b6ac-f853cc0f33df",
   "metadata": {},
   "source": [
    "#### 2. Signal Quality Feature (Moderate Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2646a883-f71a-41c8-a3b7-e14a24836c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_quality_features(signal_chunk):\n",
    "    \"\"\"\n",
    "    Extract Tier 2 features: Signal quality features (1-5ms)\n",
    "    Moderate computational cost, good discriminative power\n",
    "    \n",
    "    Args:\n",
    "        signal_chunk: 1D array of signal values\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of Tier 2 features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Zero crossing analysis - fault signals showed irregular patterns\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal_chunk)))[0]\n",
    "    features['zero_crossing_rate'] = len(zero_crossings) / len(signal_chunk)\n",
    "    \n",
    "    # Peak detection - fault signals had distinctive peak patterns\n",
    "    mean_val = np.mean(signal_chunk)\n",
    "    std_val = np.std(signal_chunk)\n",
    "    threshold = mean_val + 2 * std_val  # Dynamic threshold\n",
    "    \n",
    "    peaks_positive = np.where(signal_chunk > threshold)[0]\n",
    "    peaks_negative = np.where(signal_chunk < (mean_val - 2 * std_val))[0]\n",
    "    \n",
    "    features['peak_count'] = len(peaks_positive) + len(peaks_negative)\n",
    "    features['peak_density'] = features['peak_count'] / len(signal_chunk)\n",
    "    \n",
    "    # Signal envelope analysis - sliding window variability patterns\n",
    "    window_size = min(100, len(signal_chunk) // 10)\n",
    "    if window_size > 1:\n",
    "        envelope = []\n",
    "        for i in range(0, len(signal_chunk) - window_size + 1, window_size // 2):\n",
    "            window = signal_chunk[i:i + window_size]\n",
    "            envelope.append(np.max(np.abs(window)))\n",
    "        \n",
    "        features['envelope_mean'] = np.mean(envelope)\n",
    "        features['envelope_std'] = np.std(envelope)\n",
    "        features['envelope_variation'] = features['envelope_std'] / features['envelope_mean'] if features['envelope_mean'] > 1e-10 else 0\n",
    "    else:\n",
    "        features['envelope_mean'] = np.max(np.abs(signal_chunk))\n",
    "        features['envelope_std'] = 0\n",
    "        features['envelope_variation'] = 0\n",
    "    \n",
    "    # Simple frequency domain feature - high frequency content ratio\n",
    "    fft_vals = np.abs(np.fft.fft(signal_chunk))\n",
    "    n_samples = len(fft_vals) // 2  # Only positive frequencies\n",
    "    \n",
    "    # Split into low and high frequency bands\n",
    "    split_point = n_samples // 4  # 25% split point\n",
    "    low_freq_energy = np.sum(fft_vals[:split_point]**2)\n",
    "    high_freq_energy = np.sum(fft_vals[split_point:n_samples]**2)\n",
    "    total_energy = low_freq_energy + high_freq_energy\n",
    "    \n",
    "    features['high_freq_ratio'] = high_freq_energy / total_energy if total_energy > 1e-10 else 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ef8b8-2ff7-4809-bf63-b8c5ac6a7935",
   "metadata": {},
   "source": [
    "#### 3. Advanced Features (Comparitively slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34774012-a895-4b15-a570-018a2f8f362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_features(signal_chunk,sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract Tier 3 features: Advanced features (5-20ms)\n",
    "    Higher computational cost, use only when accuracy is critical\n",
    "    \n",
    "    Args:\n",
    "        signal_chunk: 1D array of signal values\n",
    "        sampling_rate: rate of sampling\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of Tier 3 features\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Spectral features - frequency domain analysis showed clear patterns\n",
    "    fft_vals = np.fft.fft(signal_chunk)\n",
    "    fft_freq = np.fft.fftfreq(len(signal_chunk), 1/sampling_rate)\n",
    "    \n",
    "    # Get positive frequencies only\n",
    "    positive_freq_mask = fft_freq > 0\n",
    "    frequencies = fft_freq[positive_freq_mask]\n",
    "    magnitudes = np.abs(fft_vals[positive_freq_mask])\n",
    "    power = magnitudes**2\n",
    "    \n",
    "    if np.sum(magnitudes) > 1e-10:\n",
    "        # Spectral centroid - frequency center of mass\n",
    "        features['spectral_centroid'] = np.sum(frequencies * magnitudes) / np.sum(magnitudes)\n",
    "        \n",
    "        # Dominant frequency features\n",
    "        dominant_freq_idx = np.argmax(magnitudes)\n",
    "        features['dominant_frequency'] = frequencies[dominant_freq_idx]\n",
    "        features['dominant_magnitude'] = magnitudes[dominant_freq_idx]\n",
    "        features['dominant_power_ratio'] = magnitudes[dominant_freq_idx]**2 / np.sum(power)\n",
    "    else:\n",
    "        features['spectral_centroid'] = 0\n",
    "        features['dominant_frequency'] = 0\n",
    "        features['dominant_magnitude'] = 0\n",
    "        features['dominant_power_ratio'] = 0\n",
    "    \n",
    "    # Simple wavelet analysis - 3 levels only for efficiency\n",
    "    try:\n",
    "        coeffs = pywt.wavedec(signal_chunk, 'db4', level=3)\n",
    "        \n",
    "        # Energy distribution across levels\n",
    "        total_energy = sum([np.sum(coeff**2) for coeff in coeffs])\n",
    "        \n",
    "        if total_energy > 1e-10:\n",
    "            for i, coeff in enumerate(coeffs):\n",
    "                level_name = 'approx' if i == 0 else f'detail_{i}'\n",
    "                features[f'wavelet_{level_name}_energy_ratio'] = np.sum(coeff**2) / total_energy\n",
    "        else:\n",
    "            for i in range(4):  # 3 levels + approximation\n",
    "                level_name = 'approx' if i == 0 else f'detail_{i}'\n",
    "                features[f'wavelet_{level_name}_energy_ratio'] = 0\n",
    "                \n",
    "    except:\n",
    "        # Fallback if wavelet fails\n",
    "        for i in range(4):\n",
    "            level_name = 'approx' if i == 0 else f'detail_{i}'\n",
    "            features[f'wavelet_{level_name}_energy_ratio'] = 0\n",
    "    \n",
    "    # Autocorrelation feature \n",
    "    if len(signal_chunk) > 10:\n",
    "        autocorr = np.correlate(signal_chunk, signal_chunk, mode='full')\n",
    "        autocorr = autocorr[len(autocorr)//2:]\n",
    "        autocorr = autocorr / autocorr[0] if autocorr[0] != 0 else autocorr\n",
    "        \n",
    "        # Find first minimum (indicates periodicity)\n",
    "        if len(autocorr) > 2:\n",
    "            features['autocorr_first_min'] = np.argmin(autocorr[1:]) + 1\n",
    "        else:\n",
    "            features['autocorr_first_min'] = 0\n",
    "    else:\n",
    "        features['autocorr_first_min'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105f2d5-614f-4ca3-9520-af7e7b0cb623",
   "metadata": {},
   "source": [
    "#### 4. Power System specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b721247-5347-4dba-b578-6a26421184af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_power_system_specific_features(signal_chunk,sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract power system specific features optimized for grid applications\n",
    "    \n",
    "    Args:\n",
    "        signal_chunk: 1D array of signal values\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of power system specific features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Simplified THD estimation for real-time applications\n",
    "    fft_vals = np.abs(np.fft.fft(signal_chunk))\n",
    "    fft_freq = np.fft.fftfreq(len(signal_chunk), 1/sampling_rate)\n",
    "    \n",
    "    # Look for fundamental frequency in reasonable power system range (40-70 Hz)\n",
    "    power_freq_mask = (fft_freq >= 40) & (fft_freq <= 70)\n",
    "    \n",
    "    if np.any(power_freq_mask):\n",
    "        fundamental_idx = np.argmax(fft_vals[power_freq_mask])\n",
    "        fundamental_freq = fft_freq[power_freq_mask][fundamental_idx]\n",
    "        fundamental_magnitude = fft_vals[power_freq_mask][fundamental_idx]\n",
    "        \n",
    "        # Simple harmonic estimation - 2nd and 3rd harmonics only for speed\n",
    "        harmonic_energy = 0\n",
    "        for harmonic in [2, 3]:  # Only check key harmonics\n",
    "            harmonic_freq = fundamental_freq * harmonic\n",
    "            harmonic_idx = np.argmin(np.abs(fft_freq - harmonic_freq))\n",
    "            if harmonic_idx < len(fft_vals):\n",
    "                harmonic_energy += fft_vals[harmonic_idx]**2\n",
    "        \n",
    "        features['thd_estimate'] = np.sqrt(harmonic_energy) / fundamental_magnitude if fundamental_magnitude > 1e-10 else 0\n",
    "        features['fundamental_frequency'] = fundamental_freq\n",
    "    else:\n",
    "        features['thd_estimate'] = 0\n",
    "        features['fundamental_frequency'] = 0\n",
    "    \n",
    "    # Transient detection - sudden amplitude changes\n",
    "    if len(signal_chunk) > 1:\n",
    "        diff_signal = np.diff(signal_chunk)\n",
    "        features['transient_index'] = np.std(diff_signal) / np.std(signal_chunk) if np.std(signal_chunk) > 1e-10 else 0\n",
    "        features['max_gradient'] = np.max(np.abs(diff_signal))\n",
    "    else:\n",
    "        features['transient_index'] = 0\n",
    "        features['max_gradient'] = 0\n",
    "    \n",
    "    # Voltage sag/swell detection (simplified)\n",
    "    features['amplitude_variation'] = (np.max(signal_chunk) - np.min(signal_chunk)) / np.std(signal_chunk) if np.std(signal_chunk) > 1e-10 else 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f001fa-792b-4f1e-b49c-37ca0fe97e09",
   "metadata": {},
   "source": [
    "#### Helper function for chunking signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d14597a4-8c2f-4a2d-8b56-ed16d066c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_signal(signal,chunk_size):\n",
    "    \"\"\"\n",
    "    Divide signal into chunks for feature extraction\n",
    "    \n",
    "    Args:\n",
    "        signal: 1D array of signal values\n",
    "        \n",
    "    Returns:\n",
    "        list: List of signal chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    n_chunks = len(signal) // chunk_size\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = start_idx + chunk_size\n",
    "        chunks.append(signal[start_idx:end_idx])\n",
    "    \n",
    "    # Handle remaining samples\n",
    "    if len(signal) % chunk_size != 0:\n",
    "        chunks.append(signal[n_chunks * chunk_size:])\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585a2c2-eafc-470f-9092-6394ba982b70",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b840a75-5349-4938-b939-2464f764642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_columns = signals_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea6159-97dd-4582-a052-54589d2bb667",
   "metadata": {},
   "source": [
    "##### Creating Subset of Measurements per Signal (60K/80K) & subset of samples (5K/8.7K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74daad5e-a435-48d1-8a34-bcf14841cc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 5000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_signals =signals_df.loc[10001:70000,:'4999'].copy()\n",
    "subset_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3752f115-d342-43c5-b95c-f650f24e20f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_metadata = metadata_df.loc[:4999,:].copy()\n",
    "subset_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4daa8ce-2aef-491b-901c-8f7e4d9508de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4668\n",
       "1     332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target distribution in subset\n",
    "subset_metadata['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12492b38-8f10-47e2-80bd-8bbf9390d5c8",
   "metadata": {},
   "source": [
    "### Feature Engineering at Chunk Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1c9663f-d0e8-4099-b2c7-87187d234c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing signal 1/5000\n",
      "Processing signal 101/5000\n",
      "Processing signal 201/5000\n",
      "Processing signal 301/5000\n",
      "Processing signal 401/5000\n",
      "Processing signal 501/5000\n",
      "Processing signal 601/5000\n",
      "Processing signal 701/5000\n",
      "Processing signal 801/5000\n",
      "Processing signal 901/5000\n",
      "Processing signal 1001/5000\n",
      "Processing signal 1101/5000\n",
      "Processing signal 1201/5000\n",
      "Processing signal 1301/5000\n",
      "Processing signal 1401/5000\n",
      "Processing signal 1501/5000\n",
      "Processing signal 1601/5000\n",
      "Processing signal 1701/5000\n",
      "Processing signal 1801/5000\n",
      "Processing signal 1901/5000\n",
      "Processing signal 2001/5000\n",
      "Processing signal 2101/5000\n",
      "Processing signal 2201/5000\n",
      "Processing signal 2301/5000\n",
      "Processing signal 2401/5000\n",
      "Processing signal 2501/5000\n",
      "Processing signal 2601/5000\n",
      "Processing signal 2701/5000\n",
      "Processing signal 2801/5000\n",
      "Processing signal 2901/5000\n",
      "Processing signal 3001/5000\n",
      "Processing signal 3101/5000\n",
      "Processing signal 3201/5000\n",
      "Processing signal 3301/5000\n",
      "Processing signal 3401/5000\n",
      "Processing signal 3501/5000\n",
      "Processing signal 3601/5000\n",
      "Processing signal 3701/5000\n",
      "Processing signal 3801/5000\n",
      "Processing signal 3901/5000\n",
      "Processing signal 4001/5000\n",
      "Processing signal 4101/5000\n",
      "Processing signal 4201/5000\n",
      "Processing signal 4301/5000\n",
      "Processing signal 4401/5000\n",
      "Processing signal 4501/5000\n",
      "Processing signal 4601/5000\n",
      "Processing signal 4701/5000\n",
      "Processing signal 4801/5000\n",
      "Processing signal 4901/5000\n",
      "CPU times: total: 5min 9s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "all_features = []\n",
    "signal_columns = subset_signals.columns\n",
    "for i, signal_id in enumerate(signal_columns):\n",
    "    if  i % 100 == 0:\n",
    "        print(f\"Processing signal {i+1}/{len(signal_columns)}\")\n",
    "\n",
    "    \n",
    "    signal = subset_signals[signal_id].values\n",
    "\n",
    "    # Divide signal into chunks\n",
    "    chunks = chunk_signal(signal,CHUNK_SIZE)\n",
    "    \n",
    "    feature_list = []\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        if len(chunk) < 10:  # Skip very small chunks\n",
    "            continue\n",
    "            \n",
    "        chunk_features = {}\n",
    "        chunk_features['signal_id'] = signal_id if signal_id is not None else 0\n",
    "        chunk_features['chunk_id'] = chunk_idx\n",
    "        \n",
    "        # Extract Basic (fastest, most important)\n",
    "        chunk_features.update(extract_basic_features(chunk))\n",
    "        \n",
    "        # Extract signal quality features\n",
    "        chunk_features.update(extract_signal_quality_features(chunk))\n",
    "        \n",
    "        # Extract advanced features\n",
    "        chunk_features.update(extract_advanced_features(chunk,SAMPLE_SIZE))\n",
    "        \n",
    "        # Power system specific features \n",
    "        chunk_features.update(extract_power_system_specific_features(chunk,SAMPLE_SIZE))\n",
    "        \n",
    "        feature_list.append(chunk_features)         \n",
    "    \n",
    "    \n",
    "    signal_features = pd.DataFrame(feature_list)\n",
    "    \n",
    "    # Add metadata if available\n",
    "    if subset_metadata is not None and int(signal_id) in subset_metadata['signal_id'].values:\n",
    "        meta_row = subset_metadata[subset_metadata['signal_id'] == int(signal_id)].iloc[0]\n",
    "        for _, row in signal_features.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            for col in subset_metadata.columns:\n",
    "                if col != 'signal_id':\n",
    "                    row_dict[col] = meta_row[col]\n",
    "            all_features.append(row_dict)\n",
    "    else:\n",
    "        all_features.extend(signal_features.to_dict('records'))\n",
    "\n",
    "feature_df = pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d69cbf9-58ae-4724-9045-27b6df950c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 36)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82b727f1-dfd2-49f0-9fa7-9da86e32cabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>std</th>\n",
       "      <th>rms</th>\n",
       "      <th>mean_abs</th>\n",
       "      <th>min_val</th>\n",
       "      <th>max_val</th>\n",
       "      <th>peak_to_peak</th>\n",
       "      <th>crest_factor</th>\n",
       "      <th>form_factor</th>\n",
       "      <th>...</th>\n",
       "      <th>wavelet_detail_3_energy_ratio</th>\n",
       "      <th>autocorr_first_min</th>\n",
       "      <th>thd_estimate</th>\n",
       "      <th>fundamental_frequency</th>\n",
       "      <th>transient_index</th>\n",
       "      <th>max_gradient</th>\n",
       "      <th>amplitude_variation</th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940083</td>\n",
       "      <td>5.305280</td>\n",
       "      <td>16.838</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>3.769829</td>\n",
       "      <td>0.315078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>48</td>\n",
       "      <td>0.334519</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.580357</td>\n",
       "      <td>5</td>\n",
       "      <td>6.382415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913902</td>\n",
       "      <td>4.020199</td>\n",
       "      <td>16.472</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>4.726134</td>\n",
       "      <td>0.244063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>57</td>\n",
       "      <td>0.344680</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.540460</td>\n",
       "      <td>5</td>\n",
       "      <td>6.565259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.971391</td>\n",
       "      <td>2.788548</td>\n",
       "      <td>16.220</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7.172191</td>\n",
       "      <td>0.171920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>219</td>\n",
       "      <td>0.406761</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.601921</td>\n",
       "      <td>5</td>\n",
       "      <td>7.206163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.873531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.712</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>121</td>\n",
       "      <td>2.257499</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.569174</td>\n",
       "      <td>4</td>\n",
       "      <td>6.868676</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.943478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.543</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>477</td>\n",
       "      <td>0.648326</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.600173</td>\n",
       "      <td>5</td>\n",
       "      <td>5.299540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  signal_id  chunk_id       std       rms  mean_abs  min_val  max_val  \\\n",
       "0         0         0  0.940083  5.305280    16.838       14       20   \n",
       "1         0         1  0.913902  4.020199    16.472       13       19   \n",
       "2         0         2  0.971391  2.788548    16.220       13       20   \n",
       "3         0         3  0.873531       NaN    15.712       12       18   \n",
       "4         0         4  0.943478       NaN    15.543       13       18   \n",
       "\n",
       "   peak_to_peak  crest_factor  form_factor  ...  \\\n",
       "0             6      3.769829     0.315078  ...   \n",
       "1             6      4.726134     0.244063  ...   \n",
       "2             7      7.172191     0.171920  ...   \n",
       "3             6      0.000000          NaN  ...   \n",
       "4             5      0.000000          NaN  ...   \n",
       "\n",
       "   wavelet_detail_3_energy_ratio  autocorr_first_min  thd_estimate  \\\n",
       "0                       0.001958                  48      0.334519   \n",
       "1                       0.001858                  57      0.344680   \n",
       "2                       0.002370                 219      0.406761   \n",
       "3                       0.001929                 121      2.257499   \n",
       "4                       0.002489                 477      0.648326   \n",
       "\n",
       "   fundamental_frequency  transient_index  max_gradient  amplitude_variation  \\\n",
       "0                   40.0         1.580357             5             6.382415   \n",
       "1                   40.0         1.540460             5             6.565259   \n",
       "2                   40.0         1.601921             5             7.206163   \n",
       "3                   48.0         1.569174             4             6.868676   \n",
       "4                   40.0         1.600173             5             5.299540   \n",
       "\n",
       "   id_measurement  phase  target  \n",
       "0               0      0       0  \n",
       "1               0      0       0  \n",
       "2               0      0       0  \n",
       "3               0      0       0  \n",
       "4               0      0       0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ae2dec1-1027-4f1b-84a4-79406b801b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunk_level features (Optional)\n",
    "feature_df.to_parquet(f\"{FEATURE_PATH}/features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c90a2ca-68af-4f3e-8d5e-93ee20f17c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature columns: 31\n",
      "Basic features (Fastest): 10\n",
      "Signal Quality features (fast): 7\n",
      "Advanced features (advanced): 9\n",
      "Power System features (advanced): 14\n"
     ]
    }
   ],
   "source": [
    "# Store feature names for later use\n",
    "feature_names = [col for col in feature_df.columns \n",
    "                    if col not in ['signal_id', 'chunk_id', 'target', 'id_measurement', 'phase']]\n",
    "\n",
    "print(f\"Number of feature columns: {len(feature_names)}\")\n",
    "    \n",
    "# Show feature distribution by tier\n",
    "basic_features = [f for f in feature_names if f in [\n",
    "    'std', 'rms', 'mean_abs', 'min_val', 'max_val', 'peak_to_peak', \n",
    "    'crest_factor', 'form_factor', 'mean_abs_deviation', 'variance'\n",
    "]]\n",
    "signal_quality_features = [f for f in feature_names if f in [\n",
    "    'zero_crossing_rate', 'peak_count', 'peak_density', 'envelope_mean', \n",
    "    'envelope_std', 'envelope_variation', 'high_freq_ratio'\n",
    "]]\n",
    "advanced_features = [f for f in feature_names if 'spectral' in f or 'wavelet' in f  or 'dominant' in f or 'autocorr' in f]\n",
    "power_system_features = [f for f in feature_names if f not in basic_features + signal_quality_features ]\n",
    "\n",
    "print(f\"Basic features (Fastest): {len(basic_features)}\")\n",
    "print(f\"Signal Quality features (fast): {len(signal_quality_features)}\")\n",
    "print(f\"Advanced features (advanced): {len(advanced_features)}\")\n",
    "print(f\"Power System features (advanced): {len(power_system_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a078200d-d0af-4ccf-87a4-01eab36bdea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spectral_centroid',\n",
       " 'dominant_frequency',\n",
       " 'dominant_magnitude',\n",
       " 'dominant_power_ratio',\n",
       " 'wavelet_approx_energy_ratio',\n",
       " 'wavelet_detail_1_energy_ratio',\n",
       " 'wavelet_detail_2_energy_ratio',\n",
       " 'wavelet_detail_3_energy_ratio',\n",
       " 'autocorr_first_min',\n",
       " 'thd_estimate',\n",
       " 'fundamental_frequency',\n",
       " 'transient_index',\n",
       " 'max_gradient',\n",
       " 'amplitude_variation']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_system_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99b71d-d8a3-4f14-bbe7-54ae53b0d114",
   "metadata": {},
   "source": [
    "#### Aggregating Chunk Level Features to Signal Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d70f9381-d425-4a1c-a420-e63fd2c387b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 128)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by signal_id\n",
    "grouped = feature_df.groupby('signal_id')\n",
    "aggregation_methods=['mean', 'std', 'min', 'max']\n",
    "\n",
    "aggregated_features = []\n",
    "\n",
    "for signal_id, group in grouped:\n",
    "    agg_dict = {'signal_id': signal_id}\n",
    "    \n",
    "    # Copy metadata columns \n",
    "    metadata_cols = ['target', 'id_measurement', 'phase']\n",
    "    for col in metadata_cols:\n",
    "        if col in group.columns:\n",
    "            agg_dict[col] = group[col].iloc[0]\n",
    "    \n",
    "    # Aggregate numerical features\n",
    "    numerical_cols = [col for col in feature_names if group[col].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        for method in aggregation_methods:\n",
    "            if method == 'mean':\n",
    "                agg_dict[f'{col}_mean'] = group[col].mean()\n",
    "            elif method == 'std':\n",
    "                agg_dict[f'{col}_std'] = group[col].std()\n",
    "            elif method == 'min':\n",
    "                agg_dict[f'{col}_min'] = group[col].min()\n",
    "            elif method == 'max':\n",
    "                agg_dict[f'{col}_max'] = group[col].max()\n",
    "            elif method == 'median':\n",
    "                agg_dict[f'{col}_median'] = group[col].median()\n",
    "            elif method == 'q25':\n",
    "                agg_dict[f'{col}_q25'] = group[col].quantile(0.25)\n",
    "            elif method == 'q75':\n",
    "                agg_dict[f'{col}_q75'] = group[col].quantile(0.75)\n",
    "    \n",
    "    aggregated_features.append(agg_dict)\n",
    "\n",
    "result_df = pd.DataFrame(aggregated_features)\n",
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d48be95-e0be-4563-a021-70aa3c83f1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th>std_mean</th>\n",
       "      <th>std_std</th>\n",
       "      <th>std_min</th>\n",
       "      <th>std_max</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_std</th>\n",
       "      <th>...</th>\n",
       "      <th>transient_index_min</th>\n",
       "      <th>transient_index_max</th>\n",
       "      <th>max_gradient_mean</th>\n",
       "      <th>max_gradient_std</th>\n",
       "      <th>max_gradient_min</th>\n",
       "      <th>max_gradient_max</th>\n",
       "      <th>amplitude_variation_mean</th>\n",
       "      <th>amplitude_variation_std</th>\n",
       "      <th>amplitude_variation_min</th>\n",
       "      <th>amplitude_variation_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969092</td>\n",
       "      <td>0.049162</td>\n",
       "      <td>0.860242</td>\n",
       "      <td>1.080451</td>\n",
       "      <td>7.341025</td>\n",
       "      <td>1.681216</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463640</td>\n",
       "      <td>1.655748</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>2.563543</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6.818389</td>\n",
       "      <td>2.038527</td>\n",
       "      <td>4.989672</td>\n",
       "      <td>15.518804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942165</td>\n",
       "      <td>0.043680</td>\n",
       "      <td>0.836421</td>\n",
       "      <td>1.039576</td>\n",
       "      <td>7.142753</td>\n",
       "      <td>1.804649</td>\n",
       "      <td>...</td>\n",
       "      <td>1.432772</td>\n",
       "      <td>1.630322</td>\n",
       "      <td>4.316667</td>\n",
       "      <td>0.469102</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.909052</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>4.204635</td>\n",
       "      <td>7.140720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.731215</td>\n",
       "      <td>0.848746</td>\n",
       "      <td>1.219666</td>\n",
       "      <td>7.069020</td>\n",
       "      <td>3.508683</td>\n",
       "      <td>2.074712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282088</td>\n",
       "      <td>1.609108</td>\n",
       "      <td>5.066667</td>\n",
       "      <td>12.727745</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>5.734389</td>\n",
       "      <td>2.919719</td>\n",
       "      <td>-15.560856</td>\n",
       "      <td>10.362359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1.197569</td>\n",
       "      <td>0.068980</td>\n",
       "      <td>1.043908</td>\n",
       "      <td>1.349932</td>\n",
       "      <td>6.416758</td>\n",
       "      <td>2.425201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877135</td>\n",
       "      <td>1.467941</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.656131</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5.756585</td>\n",
       "      <td>0.511805</td>\n",
       "      <td>4.741198</td>\n",
       "      <td>6.889594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706477</td>\n",
       "      <td>0.038105</td>\n",
       "      <td>0.638905</td>\n",
       "      <td>0.804366</td>\n",
       "      <td>5.061745</td>\n",
       "      <td>1.742850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845962</td>\n",
       "      <td>1.013621</td>\n",
       "      <td>2.983333</td>\n",
       "      <td>0.833446</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7.578189</td>\n",
       "      <td>1.153090</td>\n",
       "      <td>5.407796</td>\n",
       "      <td>13.675374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  signal_id  target  id_measurement  phase  std_mean   std_std   std_min  \\\n",
       "0         0       0               0      0  0.969092  0.049162  0.860242   \n",
       "1         1       0               0      1  0.942165  0.043680  0.836421   \n",
       "2        10       0               3      1  1.731215  0.848746  1.219666   \n",
       "3       100       0              33      1  1.197569  0.068980  1.043908   \n",
       "4      1000       0             333      1  0.706477  0.038105  0.638905   \n",
       "\n",
       "    std_max  rms_mean   rms_std  ...  transient_index_min  \\\n",
       "0  1.080451  7.341025  1.681216  ...             1.463640   \n",
       "1  1.039576  7.142753  1.804649  ...             1.432772   \n",
       "2  7.069020  3.508683  2.074712  ...             0.282088   \n",
       "3  1.349932  6.416758  2.425201  ...             0.877135   \n",
       "4  0.804366  5.061745  1.742850  ...             0.845962   \n",
       "\n",
       "   transient_index_max  max_gradient_mean  max_gradient_std  max_gradient_min  \\\n",
       "0             1.655748           5.933333          2.563543                 4   \n",
       "1             1.630322           4.316667          0.469102                 4   \n",
       "2             1.609108           5.066667         12.727745                 3   \n",
       "3             1.467941           3.900000          0.656131                 3   \n",
       "4             1.013621           2.983333          0.833446                 2   \n",
       "\n",
       "   max_gradient_max  amplitude_variation_mean  amplitude_variation_std  \\\n",
       "0                16                  6.818389                 2.038527   \n",
       "1                 5                  5.909052                 0.668041   \n",
       "2               101                  5.734389                 2.919719   \n",
       "3                 6                  5.756585                 0.511805   \n",
       "4                 7                  7.578189                 1.153090   \n",
       "\n",
       "   amplitude_variation_min  amplitude_variation_max  \n",
       "0                 4.989672                15.518804  \n",
       "1                 4.204635                 7.140720  \n",
       "2               -15.560856                10.362359  \n",
       "3                 4.741198                 6.889594  \n",
       "4                 5.407796                13.675374  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c8679da-b3c1-45c6-bc40-830980959506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the signal granulairty features (Optional)\n",
    "result_df.to_parquet(f\"{FEATURE_PATH}/agg_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41d17fa3-f889-4277-9f77-29a91d5ff3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['signal_id', 'target', 'id_measurement', 'phase', 'std_mean', 'std_std',\n",
       "       'std_min', 'std_max', 'rms_mean', 'rms_std',\n",
       "       ...\n",
       "       'transient_index_min', 'transient_index_max', 'max_gradient_mean',\n",
       "       'max_gradient_std', 'max_gradient_min', 'max_gradient_max',\n",
       "       'amplitude_variation_mean', 'amplitude_variation_std',\n",
       "       'amplitude_variation_min', 'amplitude_variation_max'],\n",
       "      dtype='object', length=128)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4ed11-740c-4f41-afdb-5bfdbb04060c",
   "metadata": {},
   "source": [
    "### Feature Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "217e0198-4ed7-4e14-812a-e47839728ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = result_df.copy()\n",
    "preprocessing_info = {}\n",
    "\n",
    "# Separate feature columns from metadata\n",
    "metadata_cols = ['signal_id', 'chunk_id', 'target', 'id_measurement', 'phase']\n",
    "feature_cols = [col for col in processed_df.columns if col not in metadata_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329460c-c6ad-4c8a-8d58-35d41559a1bc",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f732baf-cba8-441b-a24f-dec346276531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "processed_df[feature_cols] = processed_df[feature_cols].fillna(processed_df[feature_cols].median())\n",
    "\n",
    "preprocessing_info['missing_values_handled'] = 'Median'\n",
    "preprocessing_info['original_feature_count'] = len(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f7406-f8df-402b-811c-f6427ffc9190",
   "metadata": {},
   "source": [
    "#### Infinite Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c042fa9e-29b5-4d1a-a8da-d17a8e30ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle infinite values\n",
    "processed_df[feature_cols] = processed_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "processed_df[feature_cols] = processed_df[feature_cols].fillna(processed_df[feature_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958fd8f-420b-40ad-8724-afcaeba416bd",
   "metadata": {},
   "source": [
    "#### Select High Variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c4a98df-a97c-4cb1-92d8-35162c85c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low variance features\n",
    "variance_threshold=0.01\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "feature_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "# Fit on feature columns only\n",
    "feature_data = processed_df[feature_cols]\n",
    "selected_features = feature_selector.fit_transform(feature_data)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_feature_names = [feature_cols[i] for i in range(len(feature_cols)) \n",
    "                        if feature_selector.variances_[i] > variance_threshold]\n",
    "\n",
    "# Update feature columns\n",
    "feature_cols = selected_feature_names\n",
    "\n",
    "# Reconstruct dataframe\n",
    "temp_df = processed_df.copy()\n",
    "for i, col in enumerate(selected_feature_names):\n",
    "    temp_df[col] = selected_features[:, i]\n",
    "processed_df = temp_df\n",
    "\n",
    "preprocessing_info['features_after_variance_selection'] = len(feature_cols)\n",
    "preprocessing_info['removed_low_variance_features'] = preprocessing_info['original_feature_count'] - len(feature_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a935178-ae81-47b2-bbdf-6afaa09051d6",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cb483f6-449b-4355-aa0e-198bf96d3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "scaled_features = scaler.fit_transform(processed_df[feature_cols])\n",
    "    \n",
    "# Update dataframe with scaled features\n",
    "temp_df = processed_df.copy()\n",
    "for i, col in enumerate(feature_cols):\n",
    "    temp_df[col] = scaled_features[:, i]\n",
    "processed_df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a82663a0-5e0d-4f6e-b64a-4b8c2c44b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete:\n",
      "  Original features: 124\n",
      "  After variance selection: 92\n",
      "  Final features: 92\n"
     ]
    }
   ],
   "source": [
    "preprocessing_info['scaler_type'] = 'median'\n",
    "preprocessing_info['final_feature_count'] = len(feature_cols)\n",
    "preprocessing_info['final_feature_names'] = feature_cols\n",
    "\n",
    "print(f\"Preprocessing complete:\")\n",
    "print(f\"  Original features: {preprocessing_info['original_feature_count']}\")\n",
    "print(f\"  After variance selection: {preprocessing_info['features_after_variance_selection']}\")\n",
    "print(f\"  Final features: {preprocessing_info['final_feature_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9b06f-4fc8-4728-9a05-63fe14e636a2",
   "metadata": {},
   "source": [
    "#### Saving Final Features for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0bdee25-8677-4d5f-960a-4764b244302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_parquet(f\"{FEATURE_PATH}/final_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f25ba-d6a2-46ab-8266-3d8ec18bc627",
   "metadata": {},
   "source": [
    "### End of Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
